{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunaidRaza78/MachineLearning/blob/master/machine_learning__write_code_in_python__4__09_04_2024_14_37_49_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When working with SMILES (Simplified Molecular Input Line Entry System) strings for training generative models like a Variational Autoencoder (VAE), effective preprocessing is crucial. The SMILES notation represents the structure of a chemical species using short ASCII strings, which makes it suitable for machine learning models that process sequential data.\n",
        "\n",
        "Here's a step-by-step guide to preprocessing SMILES strings for your VAE model in Python:\n",
        "\n",
        "1. **Data Collection and Cleaning**\n",
        "\n",
        "  First, gather your dataset of SMILES strings, which might come from public chemical databases like PubChem, ChEMBL, or a proprietary dataset.\n",
        "\n",
        "- **Remove duplicates**: Ensure that the dataset does not contain duplicate entries\n",
        "to prevent bias.\n",
        "- **Data cleaning**: Some SMILES strings might be invalid or too long for practical use in training. You can use a chemistry library like RDKit to filter out invalid SMILES and possibly normalize them.\n",
        "\n",
        "2. **Tokenization**\n",
        "\n",
        "  Tokenization is the process of converting the SMILES strings into a format that can be fed into the VAE. Since SMILES are sequences of characters, each unique character can be treated as a token.\n",
        "\n",
        "- **Identify unique characters**: Parse through all SMILES strings to gather a set of all unique characters used.\n",
        "- **Create a character-to-index dictionary**: Map each character to a unique integer for model processing."
      ],
      "metadata": {
        "id": "ahfdfJ0sepqb"
      },
      "id": "ahfdfJ0sepqb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def build_vocab(smiles_list):\n",
        "    vocab = set()\n",
        "    for smiles in smiles_list:\n",
        "        vocab.update(smiles)\n",
        "    vocab = sorted(vocab)\n",
        "    char_to_index = {char: idx for idx, char in enumerate(\n",
        "        vocab, 1)}  # Start indexing from 1\n",
        "    char_to_index['<pad>'] = 0  # Add a padding character\n",
        "    return char_to_index\n",
        "\n",
        "# Example usage\n",
        "smiles_list = ['CCO', 'CCC', 'CCN']\n",
        "vocab = build_vocab(smiles_list)"
      ],
      "metadata": {
        "id": "vv2EVdYEffMd"
      },
      "id": "vv2EVdYEffMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "  Convert SMILES strings into sequences of integers using the mapping created. This transformation is necessary for neural network processing."
      ],
      "metadata": {
        "id": "xJL9tgJ8fyK1"
      },
      "id": "xJL9tgJ8fyK1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "def smiles_to_vector(smiles, char_to_index, max_length):\n",
        "    vector = [char_to_index[char] for char in smiles]\n",
        "    padded_vector = vector + [char_to_index['<pad>']] * (\n",
        "        max_length - len(vector))\n",
        "    return np.array(padded_vector)\n",
        "\n",
        "# Determine max length from your dataset\n",
        "max_length = max(len(smiles) for smiles in smiles_list)\n",
        "\n",
        "# Convert all SMILES to vectors\n",
        "smiles_vectors = np.array([smiles_to_vector(\n",
        "    smiles, vocab, max_length) for smiles in smiles_list])\n"
      ],
      "metadata": {
        "id": "nE_yfgvzfj9h"
      },
      "id": "nE_yfgvzfj9h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "4. **Data Normalization**\n",
        "\n",
        "  While traditional data normalization (e.g., mean subtraction, division by standard deviation) isn't used with SMILES strings, ensuring consistent length via padding (as done in vectorization) is essential.\n",
        "\n",
        "5. **Splitting the Dataset**\n",
        "  \n",
        "  Divide your dataset into training, validation, and test sets. This helps in training the model effectively and evaluating its performance"
      ],
      "metadata": {
        "id": "9K-TbIc3gXL_"
      },
      "id": "9K-TbIc3gXL_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test = train_test_split(\n",
        "    smiles_vectors, test_size=0.1, random_state=42)\n",
        "X_train, X_val = train_test_split(X_train, test_size=0.1, random_state=42)\n"
      ],
      "metadata": {
        "id": "IQtS4ZF9gOjy"
      },
      "id": "IQtS4ZF9gOjy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "6. **Batching and Loading**\n",
        "\n",
        "  Use a data loader to batch the data during training, which is essential for efficient training of deep learning models."
      ],
      "metadata": {
        "id": "ovkXi6rEgrGx"
      },
      "id": "ovkXi6rEgrGx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_data = TensorDataset(torch.from_numpy(X_train))\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "\n",
        "# Similarly for val_loader and test_loader"
      ],
      "metadata": {
        "id": "XngfSuKFglUZ"
      },
      "id": "XngfSuKFglUZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**7. Model Training**:\n",
        "\n",
        "  Now, with your preprocessed data, you can define your VAE model in PyTorch or TensorFlow/Keras and start the training process. Ensure to monitor both loss and any chemical validity metrics during the training.\n",
        "\n",
        "This preprocessing pipeline sets a robust foundation for training a VAE on SMILES data, aiming to generate new and valid drug-like compounds efficiently.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oVg0Sdhvg50E"
      },
      "id": "oVg0Sdhvg50E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Designing a Variational Autoencoder (VAE) to handle SMILES strings, which represent molecular structures, requires a thoughtful approach to both the encoder and decoder architectures. SMILES strings are sequences of characters, and as such, they share similarities with natural language processing (NLP). Therefore, models that excel in NLP tasks, such as recurrent neural networks (RNNs), LSTMs, GRUs, or Transformer-based models, are particularly suitable.\n",
        "\n",
        "**Encoder Design**\n",
        "\n",
        "The encoder's job is to convert the input SMILES strings into a fixed-size latent space that captures the essential information about the molecular structure.\n",
        "\n",
        "1. **Input Layer:**\n",
        "\n",
        "- **Embedding Layer**: Start with an embedding layer that converts the input tokens (characters in SMILES) into dense vectors. This layer translates the sparse, one-hot encoded vectors into a more manageable and meaningful form for the network.\n",
        "2. **Recurrent Layers**:\n",
        "\n",
        "- **GRU/LSTM**: Use GRU (Gated Recurrent Units) or LSTM (Long Short-Term Memory) layers to process the sequence data. These layers can handle the dependencies and structural characteristics in SMILES strings, which are crucial for capturing the sequential nature of the data.\n",
        "3. **Variational Layer**:\n",
        "\n",
        "- **Latent Space Representation**: After processing the sequence with RNN layers, connect to a dense layer that represents the mean (μ) and log variance (log σ²) of the latent space.\n",
        "- **Reparameterization Trick**: Implement the reparameterization trick to enable backpropagation. This involves sampling from the latent space using the generated μ and σ and adding a stochastic element by sampling from a standard normal distribution.\n"
      ],
      "metadata": {
        "id": "kLrP9XVqhwOc"
      },
      "id": "kLrP9XVqhwOc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        # Embedding layer converts token indices to dense vectors of a fixed size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU layer for processing sequences; outputs last hidden state\n",
        "        self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
        "        # Linear layers to produce means and log variance for the latent space\n",
        "        self.linear_mu = nn.Linear(rnn_units, latent_dim)\n",
        "        self.linear_var = nn.Linear(rnn_units, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convert sparse input tokens to dense embedding vectors\n",
        "        x = self.embedding(x)\n",
        "        # GRU returns output and last hidden state; '_' is unused output\n",
        "        _, h = self.rnn(x)\n",
        "        # Squeeze to remove the first dimension (batch dimension remains)\n",
        "        h = h.squeeze(0)\n",
        "        # Compute the mean and log variance for parameterizing the latent distribution\n",
        "        mu = self.linear_mu(h)\n",
        "        log_var = self.linear_var(h)\n",
        "        return mu, log_var"
      ],
      "metadata": {
        "id": "UhIVeAgYg1k-"
      },
      "id": "UhIVeAgYg1k-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Decoder Design**\n",
        "\n",
        "The decoder reconstructs SMILES strings from the latent representations, essentially mirroring the structure of the encoder but in reverse.\n",
        "\n",
        "1. **Input Layer**:\n",
        "\n",
        "- **Latent to Sequence**: Start from the latent space and expand it to the sequence length required, typically using a dense layer.\n",
        "\n",
        "2. Recurrent Layers:\n",
        "\n",
        "-**GRU/LSTM**: Similar to the encoder, use GRU or LSTM layers to generate the output sequence. The initial state of this RNN can be set from the latent vector.\n",
        "\n",
        "3. Output Layer:\n",
        "\n",
        "- Dense Layer: Use a dense layer to convert the RNN outputs to the size of the vocabulary (i.e., a score for each possible character in the SMILES vocabulary).\n",
        "- Softmax Activation: Apply a softmax layer to convert these scores into probabilities for each character in the vocabulary.\n"
      ],
      "metadata": {
        "id": "3lyDM2Q0j2iK"
      },
      "id": "3lyDM2Q0j2iK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Embedding layer to convert token indices to vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # GRU layer to generate sequences from the latent space\n",
        "        self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
        "        # Linear layer to map from RNN output to vocabulary size (for generating tokens)\n",
        "        self.dense = nn.Linear(rnn_units, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Convert sparse token indices to embeddings\n",
        "        x = self.embedding(x)\n",
        "        # Process input through GRU along with the initial hidden state\n",
        "        x, h = self.rnn(x, h)\n",
        "        # Output layer that converts RNN output to logits for each vocabulary token\n",
        "        x = self.dense(x)\n",
        "        return x, h"
      ],
      "metadata": {
        "id": "Qv3icVfcjyY0"
      },
      "id": "Qv3icVfcjyY0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Explanation and Use of the Code**\n",
        "\n",
        "- **Embedding Layer**: This layer is crucial as it translates the integer-encoded SMILES characters into dense embeddings. These embeddings capture more information per character than the integers themselves.\n",
        "\n",
        "- **GRU (Gated Recurrent Unit)**: GRUs are effective for sequence generation tasks because they can maintain information over longer sequences without the vanishing gradient problem common with standard recurrent neural networks.\n",
        "\n",
        "- **Linear Transformation for Latent Variables (mu and log_var)**: These transformations convert the last hidden state of the GRU into parameters for the latent space distribution. These parameters are used to sample and generate new data points in the latent space, maintaining continuity and enabling the generation of new, plausible SMILES strings.\n",
        "\n",
        "- **Decoder's Forward Function**: It takes an initial input (typically a start token) and the latent vector (transformed into an initial state) to begin generating a sequence. The decoder can continue generating tokens based on its previous outputs until it produces an end token or reaches a maximum sequence length.\n",
        "\n",
        "This structured and commented approach in building the VAE components clarifies how each section contributes to processing and generating SMILES strings, aiding in the development of robust generative models for chemical compounds.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVuanekooqgK"
      },
      "id": "UVuanekooqgK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assistant**\n",
        "\n",
        "Training a Variational Autoencoder (VAE) on SMILES strings involves tuning several hyperparameters to optimize model performance. The choice of these parameters can significantly affect the quality of the generated SMILES strings, their diversity, and the stability of the training process. Here’s a guide to understanding and tuning the key hyperparameters for a VAE trained on SMILES strings:\n",
        "\n",
        "1. **Learning Rate**\n",
        "- **Importance**: The learning rate controls how much to change the model in response to the estimated error each time the model weights are updated.\n",
        "- **Tuning**: Start with a standard learning rate (e.g., 0.001) and adjust based on training behavior. If the training loss fluctuates widely, consider reducing it. Use learning rate schedules or adaptive learning rate methods (like Adam) to improve convergence.\n",
        "\n",
        "2. **Batch Size**\n",
        "- **Importance**: Batch size affects the stability of the training process and can influence the convergence speed and memory utilization.\n",
        "- **Tuning**: Larger batch sizes provide a more accurate estimate of the gradient but require more memory and can sometimes lead to poorer generalization. Experiment with sizes like 32, 64, or 128 to find a balance between performance and resource usage.\n",
        "\n",
        "3. **Number of Epochs**\n",
        "- **Importance**: Determines how many times the entire dataset is passed forward and backward through the neural network.\n",
        "- **Tuning**: Set a reasonably high number to ensure the network has enough iterations to learn from the data, but use early stopping to halt training if the validation loss stops improving.\n",
        "\n",
        "4. **Latent Space Dimension**\n",
        "- **Importance**: The size of the latent space determines how much information the VAE can encode about the input data.\n",
        "- **Tuning**: Smaller dimensions force the VAE to learn more efficient representations but may miss some nuances of the data. Start with a size like 50 or 100 and adjust based on the quality of generated SMILES strings.\n",
        "\n",
        "5. **RNN Layers and Units**\n",
        "- **Importance**: The complexity of the RNN architecture influences the model’s ability to capture the dependencies in the data.\n",
        "- **Tuning**:\n",
        "  - **Layers**: More layers can model more complex patterns but increase the risk of overfitting and require more computational resources.\n",
        "  - **Units**: More units (neurons) per layer allow the model to learn more detailed representations but can slow down training. Typical sizes might range from 128 to 512 units per layer.\n",
        "\n",
        "6. **Reconstruction Loss Weight**\n",
        "- **Importance**: In VAEs, the final loss function is typically a combination of reconstruction loss and KL divergence. The weight of the reconstruction loss impacts how well the VAE learns to recreate the input data.\n",
        "- **Tuning**: Adjusting the balance between reconstruction loss and KL divergence is crucial. Too much emphasis on KL divergence can lead to overly regularized models that do not capture detailed variations in the data.\n",
        "\n",
        "7. **KL Divergence Weight (Beta)**\n",
        "- **Importance**: Controls the regularization effect in the VAE, forcing the latent distribution to approximate the prior distribution.\n",
        "- **Tuning**: A technique known as \"beta annealing\" can be useful, where the influence of KL divergence is gradually increased during training. Start with a low beta and increase it as training progresses.\n",
        "\n",
        "### Hyperparameter Tuning Techniques\n",
        "- **Grid Search**: Systematically vary parameters over a fixed grid of values to find the best combination.\n",
        "- **Random Search**: Randomly sample the hyperparameter space, often more effective and efficient than grid search.\n",
        "- **Bayesian Optimization**: Uses a probabilistic model to predict which hyperparameters might lead to better performance.\n",
        "\n",
        "Monitoring and Evaluation\n",
        "- Monitor the loss components separately to understand how well the model is learning both aspects of the VAE objective.\n",
        "- Validate the chemical validity of generated SMILES using tools like RDKit, and assess diversity and novelty through similarity metrics.\n",
        "\n",
        "\\Practical Implementation\n",
        "Consider using libraries like `optuna` or `hyperopt` for more sophisticated hyperparameter optimization strategies. These libraries can handle complex search spaces and optimize based on the model’s performance metrics efficiently.\n",
        "\n",
        "By carefully tuning these hyperparameters and continuously evaluating the output quality, you can enhance the performance of a VAE designed for generating novel SMILES strings, thereby pushing forward the boundaries in computational drug discovery."
      ],
      "metadata": {
        "id": "peCokqa5pPsD"
      },
      "id": "peCokqa5pPsD"
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implementation of the loss function for a Variational Autoencoder (VAE) specifically tailored for generating SMILES strings representing molecular structures. The focus will be on balancing reconstruction loss and KL divergence to optimize the model's performance.\n",
        "\n",
        "**Key Components of the VAE Loss Function**\n",
        "\n",
        "- **Reconstruction Loss**: Measures how well the VAE can recreate the input SMILES strings. For categorical data like SMILES, the categorical cross-entropy loss is typically used.\n",
        "\n",
        "- **KL Divergence**: Provides regularization by encouraging the latent space distributions to approximate the prior distribution (usually a standard normal distribution). It helps ensure that the latent space encodes meaningful and generalizable representations.\n",
        "\n",
        "**Implementation in PyTorch**\n",
        "\n",
        "Here’s how to implement the combined VAE loss function using PyTorch:"
      ],
      "metadata": {
        "id": "0Oc0np3ctaqA"
      },
      "id": "0Oc0np3ctaqA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    \"\"\"\n",
        "    Compute VAE loss composed of reconstruction loss and KL divergence.\n",
        "\n",
        "    Parameters:\n",
        "    - recon_x: tensor, logits from VAE decoder.\n",
        "    - x: tensor, original input data (SMILES strings indices).\n",
        "    - mu: tensor, mean from the latent space.\n",
        "    - logvar: tensor, log variance from the latent space.\n",
        "    - beta: float, scales the impact of KL divergence.\n",
        "\n",
        "    Returns:\n",
        "    - Total loss as a tensor.\n",
        "    \"\"\"\n",
        "    # Reconstruction loss: categorical cross-entropy between output and target\n",
        "    recon_loss = F.cross_entropy(\n",
        "        recon_x.view(-1, recon_x.size(2)), x.view(-1), reduction='sum')\n",
        "\n",
        "    # KL divergence: measures how closely the latent variables match a standard normal distribution\n",
        "    kl_div = -0.5 * torch.sum(\n",
        "        1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Total VAE loss\n",
        "    total_loss = recon_loss + beta * kl_div\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "3rDMh4WYoknV"
      },
      "id": "3rDMh4WYoknV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- `recon_x `should be the raw logits from the decoder (not passed through softmax).\n",
        "- `x` must be the tensor of indices representing characters in the SMILES strings.\n",
        "- `mu` and `logvar` are the outputs from the encoder, representing the parameters of the latent distribution.\n",
        "- `beta` is a tuning parameter that allows control over the trade-off between enforcing the latent distribution to match the prior and achieving accurate reconstruction.\n",
        "\n",
        "**Using the Loss Function**\n",
        "\n",
        "During training, apply this function to calculate the loss and backpropagate errors to update the model. Monitoring both components of the loss (reconstruction and KL divergence) is crucial for diagnosing model performance and ensuring effective learning.\n",
        "\n",
        "This concise setup emphasizes the practical aspects of implementing and using the VAE loss function, facilitating efficient training and optimization of your generative model for SMILES strings."
      ],
      "metadata": {
        "id": "SRl7-eVgwdrD"
      },
      "id": "SRl7-eVgwdrD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To effectively evaluate the novelty and plausibility of molecules generated by a Variational Autoencoder (VAE), you should incorporate checks for chemical validity, assess drug-likeness, compute novelty, and analyze molecular property distributions.\n",
        "\n",
        "Below is a Python code block using RDKit that encapsulates these checks:\n",
        "\n"
      ],
      "metadata": {
        "id": "NMSpZT2wxKvE"
      },
      "id": "NMSpZT2wxKvE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdMolDescriptors, Descriptors, AllChem, DataStructs\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def is_valid_smiles(smiles):\n",
        "    \"\"\"\n",
        "    Check if a SMILES string is chemically valid.\n",
        "\n",
        "    Parameters:\n",
        "    - smiles: String representing a molecule in SMILES notation.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the SMILES string can be parsed into a valid molecule;\n",
        "    False otherwise.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return mol is not None\n",
        "\n",
        "def check_druglike_properties(smiles):\n",
        "    \"\"\"\n",
        "    Assess drug-likeness of a molecule using Lipinski's Rule of Five.\n",
        "\n",
        "    Parameters:\n",
        "    - smiles: String representing a molecule in SMILES notation.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the molecule meets Lipinski's Rule of Five; False otherwise.\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if not mol:\n",
        "        return False\n",
        "    mw = Descriptors.ExactMolWt(mol)\n",
        "    logp = Descriptors.MolLogP(mol)\n",
        "    hbd = rdMolDescriptors.CalcNumHBD(mol)\n",
        "    hba = rdMolDescriptors.CalcNumHBA(mol)\n",
        "    return (mw < 500 and logp <= 5 and hbd <= 5 and hba <= 10)\n",
        "\n",
        "def compute_min_similarity(smiles, reference_smiles):\n",
        "    \"\"\"\n",
        "    Compute minimum Tanimoto similarity between a given SMILES\n",
        "    and a list of reference SMILES.\n",
        "\n",
        "    Parameters:\n",
        "    - smiles: String, SMILES notation of the molecule to compare.\n",
        "    - reference_smiles: List of strings, SMILES notations of known molecules.\n",
        "\n",
        "    Returns:\n",
        "    - float: Minimum similarity score to any molecule in the reference set.\n",
        "    \"\"\"\n",
        "    mol1 = Chem.MolFromSmiles(smiles)\n",
        "    fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, radius=2)\n",
        "    similarities = []\n",
        "    for ref_smiles in reference_smiles:\n",
        "        mol2 = Chem.MolFromSmiles(ref_smiles)\n",
        "        fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, radius=2)\n",
        "        similarities.append(DataStructs.TanimotoSimilarity(fp1, fp2))\n",
        "    return min(similarities) if similarities else None\n",
        "\n",
        "# Example usage\n",
        "generated_smiles = [\"CCO\", \"N#N\", \"C1CCCC1\", \"invalidSMILES\"]  # Hypothetical generated SMILES\n",
        "reference_smiles = [\"CCC\", \"O=C(C)Oc1ccccc1C(=O)O\", \"c1ccccc1\"]  # Known molecules\n",
        "\n",
        "valid_smiles = [sm for sm in generated_smiles if is_valid_smiles(sm)]\n",
        "druglike_smiles = [sm for sm in valid_smiles if check_druglike_properties(sm)]\n",
        "novelty_scores = {sm: compute_min_similarity(\n",
        "    sm, reference_smiles) for sm in druglike_smiles}\n",
        "\n",
        "print(\"Valid SMILES:\", valid_smiles)\n",
        "print(\"Drug-like SMILES:\", druglike_smiles)\n",
        "print(\"Novelty Scores:\", novelty_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "4tVzytr7wZ--"
      },
      "id": "4tVzytr7wZ--",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* `is_valid_smiles`: Checks chemical validity of each SMILES string.\n",
        "* `check_druglike_properties`: Applies Lipinski's Rule of Five to determine drug-likeness.\n",
        "* `compute_min_similarity`: Calculates the minimum Tanimoto similarity between generated and known molecules to assess novelty.\n",
        "\n",
        "**Usage**: The example processes a list of generated SMILES strings, filtering them based on validity and drug-likeness, then calculates novelty scores against a reference set.\n",
        "\n",
        "This implementation allows you to systematically evaluate generated molecules for their potential use in further drug development applications."
      ],
      "metadata": {
        "id": "aXgM_BRHytm3"
      },
      "id": "aXgM_BRHytm3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For implementing a Variational Autoencoder (VAE) to work with SMILES strings, you can efficiently use the combination of PyTorch for deep learning model development and RDKit for cheminformatics tasks. Here’s a concise guide and example of integrating these libraries into a VAE model:\n",
        "\n",
        "* **Deep Learning Framework**: `PyTorch` and `TensorFlow/Keras`\n",
        "\n",
        "  `PyTorch` offers dynamic computation graphs and a flexible approach to building deep learning models, which is particularly beneficial for developing custom architectures like VAEs.\n",
        "\n",
        "  `TensorFlow` offers an extensive ecosystem and Keras provides high-level APIs which are user-friendly. TensorFlow 2.x has integrated Keras deeply, making model development more intuitive.\n",
        "\n",
        "\n",
        "* **Cheminformatics Library**: `RDKit`\n",
        "\n",
        "  `RDKit` is essential for preprocessing SMILES strings to ensure chemical validity and for additional chemical informatics functionalities."
      ],
      "metadata": {
        "id": "c4jQoqGFzSA0"
      },
      "id": "c4jQoqGFzSA0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Assistant\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from rdkit import Chem\n",
        "\n",
        "# Encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)  # Embedding layer for SMILES characters\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)  # GRU to capture sequence information\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)  # Linear layer to output means of latent variables\n",
        "        self.fc_var = nn.Linear(hidden_dim, latent_dim)  # Linear layer to output log variance of latent variables\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # Convert indices to embeddings\n",
        "        _, hidden = self.rnn(embedded)  # Apply GRU\n",
        "        mu = self.fc_mu(hidden.squeeze(0))  # Get mean from the hidden state\n",
        "        log_var = self.fc_var(hidden.squeeze(0))  # Get log variance from the hidden state\n",
        "        return mu, log_var\n",
        "\n",
        "# Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim, hidden_dim)  # Map latent variables to hidden dimension\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)  # GRU for sequence generation\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)  # Output layer for generating SMILES characters\n",
        "\n",
        "    def forward(self, z, output_length):\n",
        "        hidden = self.fc(z).unsqueeze(0).repeat(output_length, 1, 1).transpose(0, 1)  # Prepare hidden states\n",
        "        output, _ = self.rnn(hidden)  # Decode the sequence\n",
        "        prediction = self.fc_out(output)  # Generate logits for next character\n",
        "        return prediction\n",
        "\n",
        "# Initialize the encoder and decoder\n",
        "input_dim = 50  # Number of unique SMILES characters\n",
        "output_length = 60  # Maximum length of SMILES strings\n",
        "hidden_dim = 128  # Hidden dimension of GRU\n",
        "latent_dim = 20  # Dimension of latent space\n",
        "\n",
        "encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "\n",
        "# Example dummy data and forward pass\n",
        "smiles_idx = torch.randint(0, input_dim, (10, output_length))  # Example indices of SMILES characters\n",
        "mu, log_var = encoder(smiles_idx)  # Encode input\n",
        "z = torch.randn_like(mu) * torch.exp(log_var / 2) + mu  # Reparameterization trick\n",
        "recon_smiles = decoder(z, output_length)  # Decode from latent space\n"
      ],
      "metadata": {
        "id": "6bOg7QXtxn8K"
      },
      "id": "6bOg7QXtxn8K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}